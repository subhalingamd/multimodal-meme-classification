{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "338Hq8xzJTur",
    "outputId": "ccc8b7de-d192-4877-a267-d4588b6a8fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Collecting contractions\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
      "Collecting anyascii\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n",
      "\u001b[K     |████████████████████████████████| 286kB 6.6MB/s \n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
      "\u001b[K     |████████████████████████████████| 327kB 9.8MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
      "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85397 sha256=48b94430d0e4f70f35c52e3d8cd387d11a91ea78a1524fd071309093fab81a07\n",
      "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
      "Successfully built pyahocorasick\n",
      "Installing collected packages: anyascii, pyahocorasick, textsearch, contractions\n",
      "Successfully installed anyascii-0.2.0 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n",
      "Collecting zeugma\n",
      "  Downloading https://files.pythonhosted.org/packages/59/38/8f57f83719027e36a61238abe1cafa55d257eaaf8e9185b2adbb5a928308/zeugma-0.48.tar.gz\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from zeugma) (1.19.5)\n",
      "Requirement already satisfied: Cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from zeugma) (0.29.22)\n",
      "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from zeugma) (1.1.5)\n",
      "Requirement already satisfied: gensim>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from zeugma) (3.6.0)\n",
      "Requirement already satisfied: scikit_learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from zeugma) (0.22.2.post1)\n",
      "Requirement already satisfied: tensorflow>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from zeugma) (2.4.1)\n",
      "Requirement already satisfied: keras>=2.1.3 in /usr/local/lib/python3.7/dist-packages (from zeugma) (2.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->zeugma) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->zeugma) (2018.9)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.5.0->zeugma) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.5.0->zeugma) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.5.0->zeugma) (5.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>=0.19.1->zeugma) (1.0.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (0.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.32.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.12)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (2.10.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (0.36.2)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.1.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (2.4.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (3.3.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (3.12.4)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->zeugma) (0.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.1.3->zeugma) (3.13)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (1.28.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (56.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (3.10.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (4.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (3.0.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (3.4.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=1.5.0->zeugma) (3.1.0)\n",
      "Building wheels for collected packages: zeugma\n",
      "  Building wheel for zeugma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for zeugma: filename=zeugma-0.48-cp37-none-any.whl size=8778 sha256=601fbee493e80d3f3f9552d65ddd368b6f3e08f0bd9d9231aa554b7fb5f0dda5\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/b5/bc/5183ac478b0071d04d3ed0c0dd4a43db94c5c8ffb317b5eb53\n",
      "Successfully built zeugma\n",
      "Installing collected packages: zeugma\n",
      "Successfully installed zeugma-0.48\n",
      "Collecting textattack\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/82/2f16ef7f22f19b3a49bbcd079dc31e53d362e1ef1299298c3eda05cf2b3a/textattack-0.2.15-py3-none-any.whl (349kB)\n",
      "\u001b[K     |████████████████████████████████| 358kB 7.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from textattack) (1.8.1+cu101)\n",
      "Collecting flair==0.6.1.post1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/49/a812ed93088ba9519cbb40eb9f52341694b31cfa126bfddcd9db3761f3ac/flair-0.6.1.post1-py3-none-any.whl (337kB)\n",
      "\u001b[K     |████████████████████████████████| 337kB 13.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from textattack) (3.2.5)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.1.5)\n",
      "Collecting word2number\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
      "Collecting bert-score>=0.3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/fb/e63e7e231a79db0489dbf7e7d0ebfb279ccb3d8216aa0d133572f784f3fa/bert_score-0.3.9-py3-none-any.whl (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
      "\u001b[?25hCollecting lemminflect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/67/d04ca98b661d4ad52b9b965c9dabb1f1a2c85541d20f8decb9a9df4e4b32/lemminflect-0.2.2-py3-none-any.whl (769kB)\n",
      "\u001b[K     |████████████████████████████████| 778kB 15.1MB/s \n",
      "\u001b[?25hCollecting language-tool-python\n",
      "  Downloading https://files.pythonhosted.org/packages/37/26/48b22ad565fd372edec3577218fb817e0e6626bf4e658033197470ad92b3/language_tool_python-2.5.3-py3-none-any.whl\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from textattack) (8.7.0)\n",
      "Collecting transformers>=3.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 32.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from textattack) (4.41.1)\n",
      "Collecting numpy<1.19.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1MB 1.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.7.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from textattack) (3.0.12)\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from textattack) (0.5.3)\n",
      "Collecting num2words\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 13.6MB/s \n",
      "\u001b[?25hCollecting lru-dict\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ea/997af58d4e6da019ad825a412f93081d9df67e9dda11cfb026a3d7cd0b6c/lru-dict-1.1.7.tar.gz\n",
      "Collecting datasets\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/d6/a3d2c55b940a7c556e88f5598b401990805fc0f0a28b2fc9870cf0b8c761/datasets-1.6.0-py3-none-any.whl (202kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 55.2MB/s \n",
      "\u001b[?25hCollecting terminaltables\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->textattack) (3.7.4.3)\n",
      "Collecting langdetect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
      "\u001b[K     |████████████████████████████████| 983kB 52.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (0.22.2.post1)\n",
      "Collecting ftfy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/b5/5da463f9c7823e0e575e9908d004e2af4b36efa8d02d3d6dad57094fcb11/ftfy-6.0.1.tar.gz (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
      "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (0.1.2)\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (3.6.4)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (2.8.1)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (3.6.0)\n",
      "Collecting janome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
      "\u001b[K     |████████████████████████████████| 19.7MB 175kB/s \n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 55.2MB/s \n",
      "\u001b[?25hCollecting mpld3==0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001b[K     |████████████████████████████████| 798kB 53.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (4.2.6)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (0.8.9)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (3.2.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (2019.12.20)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/02/be/4dd30d56a0a19619deb9bf41ba8202709fa83b1b301b876572cd6dc38117/konoha-4.6.4-py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->textattack) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack) (2018.9)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack) (20.9)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 50.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack) (3.10.1)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 50.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words->textattack) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (3.0.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (0.3.3)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
      "Collecting fsspec\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 58.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (0.70.11.1)\n",
      "Collecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 60.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.6.1.post1->textattack) (1.0.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair==0.6.1.post1->textattack) (0.2.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair==0.6.1.post1->textattack) (0.16.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair==0.6.1.post1->textattack) (2.5.1)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair==0.6.1.post1->textattack) (3.11.3)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair==0.6.1.post1->textattack) (5.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.6.1.post1->textattack) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.6.1.post1->textattack) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.6.1.post1->textattack) (1.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair==0.6.1.post1->textattack) (1.12.1)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "# import radial color histogram code, which can be found here:\n",
    "# https://github.com/gmorinan/radialColorHistogram\n",
    "from radialColorHistogram.color_density import ColorDensity\n",
    "from radialColorHistogram.radial_split import RadialSplitter\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "import regex as re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "  import contractiond\n",
    "except:\n",
    "  !pip install contractions\n",
    "  import contractions\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "# word_Lemmatized = WordNetLemmatizer()\n",
    "stop = set(stopwords.words(\"english\")) \n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "try:\n",
    "  from zeugma.embeddings import EmbeddingTransformer\n",
    "except:\n",
    "  !pip install zeugma\n",
    "  from zeugma.embeddings import EmbeddingTransformer\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "try:\n",
    "  from textattack.augmentation import WordNetAugmenter, EmbeddingAugmenter, EasyDataAugmenter\n",
    "except:\n",
    "  !pip install textattack\n",
    "  from textattack.augmentation import WordNetAugmenter, EmbeddingAugmenter, EasyDataAugmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IF_uk8u_l7d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGMm2OraIDL4"
   },
   "source": [
    "### PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 95668,
     "status": "ok",
     "timestamp": 1619257712019,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "NKWlXqTxIKRe",
    "outputId": "ac4132c5-f31b-4136-9695-3a2d940c730e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>image id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>image_2455.jpg</td>\n",
       "      <td>- It is not our fight - Are we not part of thi...</td>\n",
       "      <td>troll</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>image_3701.jpg</td>\n",
       "      <td>THAT'S THE DIFFERENCE BETWEEN YOU AND ME  YOU...</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>image_4166.png</td>\n",
       "      <td>- WHAT DO THE TITANIC AND THE SIXTH SENSE HAVE...</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>448_image_batch_2.png</td>\n",
       "      <td>\"COME ON MAN, YOU KNOW THE THING.\\r\\nJUST ASK ...</td>\n",
       "      <td>troll</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>653_image_batch_2.png</td>\n",
       "      <td>\"Those who believe without reason cannot be co...</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID               image id  ...  label label_num\n",
       "0   1         image_2455.jpg  ...  troll         2\n",
       "1   2         image_3701.jpg  ...   none         0\n",
       "2   3         image_4166.png  ...   none         0\n",
       "3   4  448_image_batch_2.png  ...  troll         2\n",
       "4   5  653_image_batch_2.png  ...   none         0\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"/content/drive/MyDrive/MTL782-Memes Classifier/dataminingmtl782/\"\n",
    "train_path = \"train_images/train_images\"\n",
    "test_path = \"test_images\"\n",
    "\n",
    "train_dt = pd.read_csv(\"/content/drive/MyDrive/MTL782-Memes Classifier/dataminingmtl782/train.csv\")\n",
    "test_dt = pd.read_csv(\"/content/drive/MyDrive/MTL782-Memes Classifier/dataminingmtl782/test.csv\")\n",
    "train_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-P8G3H1EIsOc"
   },
   "outputs": [],
   "source": [
    "def preprocess_txt(text):\n",
    "\n",
    "  caps = True if text != text.lower() else False\n",
    "  multiline = True if '\\n' in text else False\n",
    "  bullet = True if '- ' in text else False\n",
    "\n",
    "  text = text.lower()   # ??\n",
    "  text = re.sub(r\"\\n\",\" \",text)\n",
    "  text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)\n",
    "  text = re.sub(r'(([^\\s]+\\.com)|([^\\s]+\\.net)|([^\\s]+\\.in))', '', text)\n",
    "  text = re.sub(r'http\\S+', '', text)\n",
    "  \n",
    "  # contractions fis\n",
    "  expanded_words = []\n",
    "  for word in text.split():\n",
    "    expanded_words.append(contractions.fix(word))\n",
    "  text = \" \".join(expanded_words)\n",
    "\n",
    "  \n",
    "  #text = text.replace(\"@\",\"@ \")\n",
    "  text = re.sub(r'[^a-zA-Z0-9]', ' ', text) # ????\n",
    "  ## text = re.sub('@[^\\s]+','',text)\n",
    "  #####text = tweet_tokenizer.tokenize(text)\n",
    "  \n",
    "  # Single character removal\n",
    "  text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "  # Removing multiple spaces\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  \n",
    "  text = word_tokenize(text)\n",
    "\n",
    "  text = [x for x in text if x not in stop]\n",
    "  \n",
    "  #text = [stemmer.stem(x) for x in text]\n",
    "\n",
    "  ##text = [\"<CAPS>\"] + text if caps else text\n",
    "  text = [\"<MULTILINE>\"] + text if multiline else text\n",
    "  ##text = [\"<BULLET>\"] + text if bullet else text\n",
    "\n",
    "\n",
    "  text = \" \".join(text)\n",
    "\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "def preprocess_txt_mini(text):\n",
    "\n",
    "  text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)\n",
    "  text = re.sub(r'(([^\\s]+\\.com)|([^\\s]+\\.net)|([^\\s]+\\.in))', '', text)\n",
    "  text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "  text = re.sub(r'[^a-zA-Z0-9 \\.,\\?\\\\\\'\\\"\\:\\;\\#\\%\\!\\&\\-\\@\\n]', ' ', text) # ????\n",
    "\n",
    "  return text\n",
    "\n",
    "\n",
    "def tokenize(text): \n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "def stem(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "en_stopwords = set(stopwords.words(\"english\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMNrhjVlISm0"
   },
   "outputs": [],
   "source": [
    "def get_files_labels(subpath,csv):\n",
    "    \"\"\"Gets paths images within a folder, and their labels\"\"\"\n",
    "    img_paths, img_labels, texts = [], [], []\n",
    "    df = pd.read_csv(root+csv)\n",
    "    i=0\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "          img, text, label = row[\"image id\"], row[\"text\"], int(row[\"label_num\"])\n",
    "        except:\n",
    "          img, text, label = row[\"image id\"], row[\"text\"], -1\n",
    "        # print(img_,\": \",text_)\n",
    "        \n",
    "        ## Change this for img....>>>\n",
    "        if not (i in [858, 1986, 1990] and csv==\"train.csv\"):\n",
    "        #### if True:\n",
    "          img_paths.append(root+subpath+'/'+img.strip())\n",
    "          img_labels.append(label)\n",
    "          ###texts.append(preprocess_txt(text))\n",
    "          ###texts.append(text)\n",
    "          texts.append(preprocess_txt_mini(text))\n",
    "        i+=1\n",
    "        #print('\\r images: {} labels : {}'.format(len(img_paths), len(img_labels)) )\n",
    "    return img_paths, img_labels, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcowII-vJCr9"
   },
   "outputs": [],
   "source": [
    "files_, y_, texts_ = get_files_labels(train_path,\"train.csv\")\n",
    "files_test, y_test, texts_test = get_files_labels(test_path,\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 979,
     "status": "ok",
     "timestamp": 1619259227937,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "_UAyrOsRKizr",
    "outputId": "3f45dc5a-afb2-4e01-cd2f-67ff0cbbb691"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/content/drive/MyDrive/MTL782-Memes Classifier/dataminingmtl782/train_images/train_images/image_6373.png',\n",
       "       'When you finally drop that toxic girl in your life and you start glowing  The Internet Scavengers'],\n",
       "      dtype='<U545')"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    np.column_stack([files_, texts_]), y_, test_size=0.1, random_state=42, stratify=y_, shuffle=True)\n",
    "X_test = np.column_stack([files_test,texts_test])\n",
    "\n",
    "texts_[0]\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmXYldwzDhar"
   },
   "outputs": [],
   "source": [
    "##aug = EmbeddingAugmenter()\n",
    "##aug.augment(texts_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1619259231875,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "XWKbpNtdH9LJ",
    "outputId": "9b7c5997-8592-44b2-935b-e67c24bd0eda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hawaii from New York!', \"I'm ok!\"]"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def newaug(sentences,aug = EmbeddingAugmenter(),easy=False ):\n",
    "  new_sentences = []\n",
    "  for sentence in sentences:\n",
    "    if easy:\n",
    "      sen = aug.augment(sentence)\n",
    "      idx,le = 0,0\n",
    "      for i in range(len(sen)):\n",
    "        if len(sen[i]) > le:\n",
    "          le = len(sen[i])\n",
    "          idx = i\n",
    "      new_sentences.append(sen[idx])     \n",
    "    else:\n",
    "      new_sentences.append(aug.augment(sentence)[0])\n",
    "  return new_sentences\n",
    "\n",
    "# newaug(X_train[:,1])[0]\n",
    "newaug([\"Hi from New York!\",\"I'm fine!\"],WordNetAugmenter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358519,
     "status": "ok",
     "timestamp": 1619259598772,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "gHr0nhuWH9q7",
    "outputId": "6f36bb09-a1d3-4fcf-9e57-01aa36c58168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done1\n"
     ]
    }
   ],
   "source": [
    "#X_train_wn = newaug(X_train[:,1], aug=WordNetAugmenter())\n",
    "#print(\"Done0\")\n",
    "X_train_emb = newaug(X_train[:,1], aug=EmbeddingAugmenter())\n",
    "print(\"Done1\")\n",
    "#X_train_ed = newaug(X_train[:,1], aug=EasyDataAugmenter(), easy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKdZWOrpJh67"
   },
   "outputs": [],
   "source": [
    "## print(X_train[0,1])\n",
    "for i in range(X_train.shape[0]):\n",
    "  X_train[i,1] = preprocess_txt(X_train[i,1])\n",
    "\n",
    "for i in range(X_val.shape[0]):\n",
    "  X_val[i,1] = preprocess_txt(X_val[i,1])\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "  X_test[i,1] = preprocess_txt(X_test[i,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-PZ2HyXJv8E"
   },
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "  #X_train_wn[i] = preprocess_txt(X_train_wn[i])\n",
    "  X_train_emb[i] = preprocess_txt(X_train_emb[i])\n",
    "  #X_train_ed[i] = preprocess_txt(X_train_ed[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_cKzwlzR2O0"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "open_file = open(\"X_train_emb1.pkl\", \"rb\")\n",
    "X_train_emb = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "\n",
    "open_file = open(\"X_train_wn.pkl\", \"rb\")\n",
    "X_train_wn = pickle.load(open_file)\n",
    "open_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYSyj-mrkkZM"
   },
   "outputs": [],
   "source": [
    "def augment(sentences,n):\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    new_sentences = []\n",
    "    for _ in range(n):\n",
    "      for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        random.shuffle(words)\n",
    "        new_sentences.append(' '.join(words))\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIu3euxqqetW"
   },
   "outputs": [],
   "source": [
    "def get_synonyms_lexicon(path=\"ppdb-xl.txt\"):\n",
    "    synonyms_lexicon = {}\n",
    "    text_entries = [l.strip() for l in open(path).readlines()]\n",
    "    for e in text_entries:\n",
    "        e = e.split(' ')\n",
    "        k = e[0]\n",
    "        v = e[1:len(e)]\n",
    "        synonyms_lexicon[k] = v\n",
    "    return synonyms_lexicon\n",
    "\n",
    "synonyms = get_synonyms_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wmg4AvXQqylH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1906,
     "status": "ok",
     "timestamp": 1619258604235,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "gPkss-5UlmKq",
    "outputId": "86504165-dda6-4610-ece4-2fd8ba02bb62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('finally drop toxic girl life start glowing internet scavengers',\n",
       " 'girl glowing internet life scavengers toxic start finally drop')"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2 = augment(X_train[:,1],1)\n",
    "X_train[:,1][0],X_train2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 1883,
     "status": "ok",
     "timestamp": 1619258604236,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "0MUym9-1q3pB",
    "outputId": "7c232f89-3e20-4278-9eb6-dd1be4c72f91"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'finally decline toxic girl living start glowing web scavengers'"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1 = []\n",
    "for sentence in X_train[:,1]:\n",
    "  new_sentence = []\n",
    "  for word in sentence.split():\n",
    "    if word in synonyms.keys():\n",
    "      new_sentence.append(synonyms[word][0])\n",
    "    else:\n",
    "      new_sentence.append(word)\n",
    "  X_train1.append(\" \".join(new_sentence))\n",
    "\n",
    "X_train1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pRK0oQpTe4A"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54414,
     "status": "ok",
     "timestamp": 1619257856507,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "XNVsHY9eTgDP",
    "outputId": "a39ab088-9c43-4050-bf17-1f573376910c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/MyDrive/colab/data/glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "#! unzip /content/drive/MyDrive/colab/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mps7858uSyPG"
   },
   "outputs": [],
   "source": [
    "#X_train__ = X_train.copy()\n",
    "#X_val__ = X_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUUcPSPFWeg6"
   },
   "outputs": [],
   "source": [
    "X_train = list(X_train__[:,1]) + X_train1 + X_train_wn  + X_train_emb\n",
    "X_val = list(X_val__[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 1459,
     "status": "ok",
     "timestamp": 1619259883558,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "1d4NAm5BXagS",
    "outputId": "faf0f61c-4964-477b-fabf-928b5855ddb4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'finally drop toxic girl life start glowing internet scavengers'"
      ]
     },
     "execution_count": 106,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOpvpkupSyfH"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf65eF9nWbYk"
   },
   "outputs": [],
   "source": [
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 48\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbhwQHc8XQlL"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.50d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbSYG-5LXsUt"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((vocab_size, 50))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ca0vk96Xwik"
   },
   "outputs": [],
   "source": [
    "from metrics import precision, recall, f1\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(16))\n",
    "#model.add(Dense(48,activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1619262525178,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "NxxeTEm1Y6iQ",
    "outputId": "16e5fc5a-47ff-4842-f4d9-87607fcb72b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 48, 50)            388550    \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 16)                4288      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 392,889\n",
      "Trainable params: 4,339\n",
      "Non-trainable params: 388,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mn9Qt81NY9uI"
   },
   "outputs": [],
   "source": [
    "history = model.fit(np.array(X_train),np.array( 4*[[x==0,x==1,x==2] for x in y_train]), batch_size=16, epochs=64, verbose=1, \n",
    "                    validation_data=(np.array(X_val),np.array([[x==0,x==1,x==2] for x in y_val])) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IawhWsrEZFaJ"
   },
   "outputs": [],
   "source": [
    "y_val_pred_ = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLziR5uRiC3C"
   },
   "outputs": [],
   "source": [
    "y_val_pred = np.argmax(y_val_pred_,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1028,
     "status": "ok",
     "timestamp": 1619262035856,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "S6BJAdGEiFxE",
    "outputId": "03a99cf9-04f9-4aaa-9d28-899242f95c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.10      0.17        60\n",
      "           1       0.51      0.48      0.49        69\n",
      "           2       0.39      0.69      0.49        70\n",
      "\n",
      "    accuracy                           0.44       199\n",
      "   macro avg       0.50      0.42      0.39       199\n",
      "weighted avg       0.49      0.44      0.40       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report([x for x in y_val], y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpbDwtz5XZNb"
   },
   "outputs": [],
   "source": [
    "model.save(\"lstm_len48_4_f40_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eom_gtbmiTqf"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\"\"\"\n",
    "open_file = open(\"X_train_emb.pkl\", \"wb\")\n",
    "pickle.dump(X_train_emb, open_file)\n",
    "open_file.close()\n",
    "\"\"\"\n",
    "\n",
    "open_file = open(\"X_train_emb.pkl\", \"rb\")\n",
    "X_train_emb = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "#loaded_list==X_train_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8MhmrH7yFrG"
   },
   "outputs": [],
   "source": [
    "open_file = open(\"lstm__X_test.pkl\", \"wb\")\n",
    "pickle.dump(X_test, open_file)\n",
    "open_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1619263362432,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "eoNnLSGQnQ-q",
    "outputId": "1616d339-07bc-4544-d802-05b85816d383"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2297,   69,  866, 2298, 1291,   14,  825,  577, 2299,  410, 2300,\n",
       "       1770, 4900,   45,  101, 2301,  101, 2302, 2303,  118,  867,  242,\n",
       "        868,  242, 2304,  101, 2305, 2306, 2307, 2308,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 200,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ts537E6eZVQv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44q2OJfIZV7-"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "open_file = open(\"lstm__X_train_wn.pkl\", \"wb\")\n",
    "pickle.dump(X_train_wn, open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(\"lstm__X_train_emb.pkl\", \"wb\")\n",
    "pickle.dump(X_train_emb, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1619263658578,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "J9ksxuRpSgeQ",
    "outputId": "604f2750-bd57-485b-ddcd-adb6d8744294"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 113,  228, 1017, 2033, 1624, 4534,    8, 1580, 3612,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0], dtype=int32),\n",
       " 'sep',\n",
       " array([  16,   41, 1386, 4577, 1686,   89,   97,  262, 4577,  441,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 208,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[0], 'sep', X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swLFmIstl876"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J5M-c7CTmfWN"
   },
   "outputs": [],
   "source": [
    "X_val = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "executionInfo": {
     "elapsed": 877,
     "status": "error",
     "timestamp": 1619263199734,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "Nu71x1m1miQC",
    "outputId": "3ae53297-3a00-4c13-fbe5-17bf0e6c1d63"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-cf290153e199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pT4QSeR1m5VD"
   },
   "outputs": [],
   "source": [
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1619264702896,
     "user": {
      "displayName": "Subhalingam D",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg86bQ1NVmL-U0-lCsAdMpCA_G3d9fxrw_PlOwtJg=s64",
      "userId": "08059846138458314824"
     },
     "user_tz": -330
    },
    "id": "ctgOoZT_oQUL",
    "outputId": "a4457788-b426-45c0-b32f-4ab204508588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.10      0.17        60\n",
      "           1       0.51      0.48      0.49        69\n",
      "           2       0.39      0.69      0.49        70\n",
      "\n",
      "    accuracy                           0.44       199\n",
      "   macro avg       0.50      0.42      0.39       199\n",
      "weighted avg       0.49      0.44      0.40       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "mod = load_model(\"/content/lstm_len48_4_f40_2.h5\",custom_objects=\n",
    "                 {'precision':precision,\n",
    "                  'recall':recall,\n",
    "                  'f1':f1})\n",
    "y_val_pred_ = mod.predict(X_val)\n",
    "y_val_pred = np.argmax(y_val_pred_,axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report([x for x in y_val], y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoX0JJsLpOoq"
   },
   "outputs": [],
   "source": [
    "open_file = open(\"lstm__val_pred_2.pkl\", \"wb\")\n",
    "pickle.dump(y_val_pred_, open_file)\n",
    "open_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgLuEahWsWKC"
   },
   "outputs": [],
   "source": [
    "y_test_pred_ = mod.predict(X_test)\n",
    "\n",
    "open_file = open(\"lstm__test_pred_2.pkl\", \"wb\")\n",
    "pickle.dump(y_test_pred_, open_file)\n",
    "open_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPmNMKqysgSR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyONVCd+zArIinkC39imCeVN",
   "collapsed_sections": [],
   "mount_file_id": "1RpCeb7OsK8HX-dYzBpUedcI2-O8Rp1mw",
   "name": "Copy of [new]mtl782_3.ipynb",
   "provenance": [
    {
     "file_id": "1SN44hCEbm3-IwAIDEMD4LRZgzRZwcDcm",
     "timestamp": 1619155191498
    },
    {
     "file_id": "1Nwrired9lZZBWogCh1XFOvbChvhVGlqZ",
     "timestamp": 1618296278069
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
